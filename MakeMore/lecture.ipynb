{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a lot of example packed in just one single word\n",
    "# isabella ->\n",
    "# i is likely to come first\n",
    "# s is likely to come next, after i\n",
    "# ... end is likely to come next, after isabella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a <E>\n"
     ]
    }
   ],
   "source": [
    "# get consecutive element\n",
    "for w in words[:1]:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip (chs, chs[1:]):\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627\n"
     ]
    }
   ],
   "source": [
    "# generate a dictionary that can map this bigrams\n",
    "bigrams = {}\n",
    "for w in words:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        bigrams[bigram] = bigrams.get(bigram, 0) + 1\n",
    "\n",
    "print(len(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('n', '<E>'), 6763),\n",
       " (('a', '<E>'), 6640),\n",
       " (('a', 'n'), 5438),\n",
       " (('<S>', 'a'), 4410),\n",
       " (('e', '<E>'), 3983),\n",
       " (('a', 'r'), 3264),\n",
       " (('e', 'l'), 3248),\n",
       " (('r', 'i'), 3033),\n",
       " (('n', 'a'), 2977),\n",
       " (('<S>', 'k'), 2963),\n",
       " (('l', 'e'), 2921),\n",
       " (('e', 'n'), 2675),\n",
       " (('l', 'a'), 2623),\n",
       " (('m', 'a'), 2590),\n",
       " (('<S>', 'm'), 2538),\n",
       " (('a', 'l'), 2528),\n",
       " (('i', '<E>'), 2489),\n",
       " (('l', 'i'), 2480),\n",
       " (('i', 'a'), 2445),\n",
       " (('<S>', 'j'), 2422),\n",
       " (('o', 'n'), 2411),\n",
       " (('h', '<E>'), 2409),\n",
       " (('r', 'a'), 2356),\n",
       " (('a', 'h'), 2332),\n",
       " (('h', 'a'), 2244),\n",
       " (('y', 'a'), 2143),\n",
       " (('i', 'n'), 2126),\n",
       " (('<S>', 's'), 2055),\n",
       " (('a', 'y'), 2050),\n",
       " (('y', '<E>'), 2007),\n",
       " (('e', 'r'), 1958),\n",
       " (('n', 'n'), 1906),\n",
       " (('y', 'n'), 1826),\n",
       " (('k', 'a'), 1731),\n",
       " (('n', 'i'), 1725),\n",
       " (('r', 'e'), 1697),\n",
       " (('<S>', 'd'), 1690),\n",
       " (('i', 'e'), 1653),\n",
       " (('a', 'i'), 1650),\n",
       " (('<S>', 'r'), 1639),\n",
       " (('a', 'm'), 1634),\n",
       " (('l', 'y'), 1588),\n",
       " (('<S>', 'l'), 1572),\n",
       " (('<S>', 'c'), 1542),\n",
       " (('<S>', 'e'), 1531),\n",
       " (('j', 'a'), 1473),\n",
       " (('r', '<E>'), 1377),\n",
       " (('n', 'e'), 1359),\n",
       " (('l', 'l'), 1345),\n",
       " (('i', 'l'), 1345),\n",
       " (('i', 's'), 1316),\n",
       " (('l', '<E>'), 1314),\n",
       " (('<S>', 't'), 1308),\n",
       " (('<S>', 'b'), 1306),\n",
       " (('d', 'a'), 1303),\n",
       " (('s', 'h'), 1285),\n",
       " (('d', 'e'), 1283),\n",
       " (('e', 'e'), 1271),\n",
       " (('m', 'i'), 1256),\n",
       " (('s', 'a'), 1201),\n",
       " (('s', '<E>'), 1169),\n",
       " (('<S>', 'n'), 1146),\n",
       " (('a', 's'), 1118),\n",
       " (('y', 'l'), 1104),\n",
       " (('e', 'y'), 1070),\n",
       " (('o', 'r'), 1059),\n",
       " (('a', 'd'), 1042),\n",
       " (('t', 'a'), 1027),\n",
       " (('<S>', 'z'), 929),\n",
       " (('v', 'i'), 911),\n",
       " (('k', 'e'), 895),\n",
       " (('s', 'e'), 884),\n",
       " (('<S>', 'h'), 874),\n",
       " (('r', 'o'), 869),\n",
       " (('e', 's'), 861),\n",
       " (('z', 'a'), 860),\n",
       " (('o', '<E>'), 855),\n",
       " (('i', 'r'), 849),\n",
       " (('b', 'r'), 842),\n",
       " (('a', 'v'), 834),\n",
       " (('m', 'e'), 818),\n",
       " (('e', 'i'), 818),\n",
       " (('c', 'a'), 815),\n",
       " (('i', 'y'), 779),\n",
       " (('r', 'y'), 773),\n",
       " (('e', 'm'), 769),\n",
       " (('s', 't'), 765),\n",
       " (('h', 'i'), 729),\n",
       " (('t', 'e'), 716),\n",
       " (('n', 'd'), 704),\n",
       " (('l', 'o'), 692),\n",
       " (('a', 'e'), 692),\n",
       " (('a', 't'), 687),\n",
       " (('s', 'i'), 684),\n",
       " (('e', 'a'), 679),\n",
       " (('d', 'i'), 674),\n",
       " (('h', 'e'), 674),\n",
       " (('<S>', 'g'), 669),\n",
       " (('t', 'o'), 667),\n",
       " (('c', 'h'), 664),\n",
       " (('b', 'e'), 655),\n",
       " (('t', 'h'), 647),\n",
       " (('v', 'a'), 642),\n",
       " (('o', 'l'), 619),\n",
       " (('<S>', 'i'), 591),\n",
       " (('i', 'o'), 588),\n",
       " (('e', 't'), 580),\n",
       " (('v', 'e'), 568),\n",
       " (('a', 'k'), 568),\n",
       " (('a', 'a'), 556),\n",
       " (('c', 'e'), 551),\n",
       " (('a', 'b'), 541),\n",
       " (('i', 't'), 541),\n",
       " (('<S>', 'y'), 535),\n",
       " (('t', 'i'), 532),\n",
       " (('s', 'o'), 531),\n",
       " (('m', '<E>'), 516),\n",
       " (('d', '<E>'), 516),\n",
       " (('<S>', 'p'), 515),\n",
       " (('i', 'c'), 509),\n",
       " (('k', 'i'), 509),\n",
       " (('o', 's'), 504),\n",
       " (('n', 'o'), 496),\n",
       " (('t', '<E>'), 483),\n",
       " (('j', 'o'), 479),\n",
       " (('u', 's'), 474),\n",
       " (('a', 'c'), 470),\n",
       " (('n', 'y'), 465),\n",
       " (('e', 'v'), 463),\n",
       " (('s', 's'), 461),\n",
       " (('m', 'o'), 452),\n",
       " (('i', 'k'), 445),\n",
       " (('n', 't'), 443),\n",
       " (('i', 'd'), 440),\n",
       " (('j', 'e'), 440),\n",
       " (('a', 'z'), 435),\n",
       " (('i', 'g'), 428),\n",
       " (('i', 'm'), 427),\n",
       " (('r', 'r'), 425),\n",
       " (('d', 'r'), 424),\n",
       " (('<S>', 'f'), 417),\n",
       " (('u', 'r'), 414),\n",
       " (('r', 'l'), 413),\n",
       " (('y', 's'), 401),\n",
       " (('<S>', 'o'), 394),\n",
       " (('e', 'd'), 384),\n",
       " (('a', 'u'), 381),\n",
       " (('c', 'o'), 380),\n",
       " (('k', 'y'), 379),\n",
       " (('d', 'o'), 378),\n",
       " (('<S>', 'v'), 376),\n",
       " (('t', 't'), 374),\n",
       " (('z', 'e'), 373),\n",
       " (('z', 'i'), 364),\n",
       " (('k', '<E>'), 363),\n",
       " (('g', 'h'), 360),\n",
       " (('t', 'r'), 352),\n",
       " (('k', 'o'), 344),\n",
       " (('t', 'y'), 341),\n",
       " (('g', 'e'), 334),\n",
       " (('g', 'a'), 330),\n",
       " (('l', 'u'), 324),\n",
       " (('b', 'a'), 321),\n",
       " (('d', 'y'), 317),\n",
       " (('c', 'k'), 316),\n",
       " (('<S>', 'w'), 307),\n",
       " (('k', 'h'), 307),\n",
       " (('u', 'l'), 301),\n",
       " (('y', 'e'), 301),\n",
       " (('y', 'r'), 291),\n",
       " (('m', 'y'), 287),\n",
       " (('h', 'o'), 287),\n",
       " (('w', 'a'), 280),\n",
       " (('s', 'l'), 279),\n",
       " (('n', 's'), 278),\n",
       " (('i', 'z'), 277),\n",
       " (('u', 'n'), 275),\n",
       " (('o', 'u'), 275),\n",
       " (('n', 'g'), 273),\n",
       " (('y', 'd'), 272),\n",
       " (('c', 'i'), 271),\n",
       " (('y', 'o'), 271),\n",
       " (('i', 'v'), 269),\n",
       " (('e', 'o'), 269),\n",
       " (('o', 'm'), 261),\n",
       " (('r', 'u'), 252),\n",
       " (('f', 'a'), 242),\n",
       " (('b', 'i'), 217),\n",
       " (('s', 'y'), 215),\n",
       " (('n', 'c'), 213),\n",
       " (('h', 'y'), 213),\n",
       " (('p', 'a'), 209),\n",
       " (('r', 't'), 208),\n",
       " (('q', 'u'), 206),\n",
       " (('p', 'h'), 204),\n",
       " (('h', 'r'), 204),\n",
       " (('j', 'u'), 202),\n",
       " (('g', 'r'), 201),\n",
       " (('p', 'e'), 197),\n",
       " (('n', 'l'), 195),\n",
       " (('y', 'i'), 192),\n",
       " (('g', 'i'), 190),\n",
       " (('o', 'd'), 190),\n",
       " (('r', 's'), 190),\n",
       " (('r', 'd'), 187),\n",
       " (('h', 'l'), 185),\n",
       " (('s', 'u'), 185),\n",
       " (('a', 'x'), 182),\n",
       " (('e', 'z'), 181),\n",
       " (('e', 'k'), 178),\n",
       " (('o', 'v'), 176),\n",
       " (('a', 'j'), 175),\n",
       " (('o', 'h'), 171),\n",
       " (('u', 'e'), 169),\n",
       " (('m', 'm'), 168),\n",
       " (('a', 'g'), 168),\n",
       " (('h', 'u'), 166),\n",
       " (('x', '<E>'), 164),\n",
       " (('u', 'a'), 163),\n",
       " (('r', 'm'), 162),\n",
       " (('a', 'w'), 161),\n",
       " (('f', 'i'), 160),\n",
       " (('z', '<E>'), 160),\n",
       " (('u', '<E>'), 155),\n",
       " (('u', 'm'), 154),\n",
       " (('e', 'c'), 153),\n",
       " (('v', 'o'), 153),\n",
       " (('e', 'h'), 152),\n",
       " (('p', 'r'), 151),\n",
       " (('d', 'd'), 149),\n",
       " (('o', 'a'), 149),\n",
       " (('w', 'e'), 149),\n",
       " (('w', 'i'), 148),\n",
       " (('y', 'm'), 148),\n",
       " (('z', 'y'), 147),\n",
       " (('n', 'z'), 145),\n",
       " (('y', 'u'), 141),\n",
       " (('r', 'n'), 140),\n",
       " (('o', 'b'), 140),\n",
       " (('k', 'l'), 139),\n",
       " (('m', 'u'), 139),\n",
       " (('l', 'd'), 138),\n",
       " (('h', 'n'), 138),\n",
       " (('u', 'd'), 136),\n",
       " (('<S>', 'x'), 134),\n",
       " (('t', 'l'), 134),\n",
       " (('a', 'f'), 134),\n",
       " (('o', 'e'), 132),\n",
       " (('e', 'x'), 132),\n",
       " (('e', 'g'), 125),\n",
       " (('f', 'e'), 123),\n",
       " (('z', 'l'), 123),\n",
       " (('u', 'i'), 121),\n",
       " (('v', 'y'), 121),\n",
       " (('e', 'b'), 121),\n",
       " (('r', 'h'), 121),\n",
       " (('j', 'i'), 119),\n",
       " (('o', 't'), 118),\n",
       " (('d', 'h'), 118),\n",
       " (('h', 'm'), 117),\n",
       " (('c', 'l'), 116),\n",
       " (('o', 'o'), 115),\n",
       " (('y', 'c'), 115),\n",
       " (('o', 'w'), 114),\n",
       " (('o', 'c'), 114),\n",
       " (('f', 'r'), 114),\n",
       " (('b', '<E>'), 114),\n",
       " (('m', 'b'), 112),\n",
       " (('z', 'o'), 110),\n",
       " (('i', 'b'), 110),\n",
       " (('i', 'u'), 109),\n",
       " (('k', 'r'), 109),\n",
       " (('g', '<E>'), 108),\n",
       " (('y', 'v'), 106),\n",
       " (('t', 'z'), 105),\n",
       " (('b', 'o'), 105),\n",
       " (('c', 'y'), 104),\n",
       " (('y', 't'), 104),\n",
       " (('u', 'b'), 103),\n",
       " (('u', 'c'), 103),\n",
       " (('x', 'a'), 103),\n",
       " (('b', 'l'), 103),\n",
       " (('o', 'y'), 103),\n",
       " (('x', 'i'), 102),\n",
       " (('i', 'f'), 101),\n",
       " (('r', 'c'), 99),\n",
       " (('c', '<E>'), 97),\n",
       " (('m', 'r'), 97),\n",
       " (('n', 'u'), 96),\n",
       " (('o', 'p'), 95),\n",
       " (('i', 'h'), 95),\n",
       " (('k', 's'), 95),\n",
       " (('l', 's'), 94),\n",
       " (('u', 'k'), 93),\n",
       " (('<S>', 'q'), 92),\n",
       " (('d', 'u'), 92),\n",
       " (('s', 'm'), 90),\n",
       " (('r', 'k'), 90),\n",
       " (('i', 'x'), 89),\n",
       " (('v', '<E>'), 88),\n",
       " (('y', 'k'), 86),\n",
       " (('u', 'w'), 86),\n",
       " (('g', 'u'), 85),\n",
       " (('b', 'y'), 83),\n",
       " (('e', 'p'), 83),\n",
       " (('g', 'o'), 83),\n",
       " (('s', 'k'), 82),\n",
       " (('u', 't'), 82),\n",
       " (('a', 'p'), 82),\n",
       " (('e', 'f'), 82),\n",
       " (('i', 'i'), 82),\n",
       " (('r', 'v'), 80),\n",
       " (('f', '<E>'), 80),\n",
       " (('t', 'u'), 78),\n",
       " (('y', 'z'), 78),\n",
       " (('<S>', 'u'), 78),\n",
       " (('l', 't'), 77),\n",
       " (('r', 'g'), 76),\n",
       " (('c', 'r'), 76),\n",
       " (('i', 'j'), 76),\n",
       " (('w', 'y'), 73),\n",
       " (('z', 'u'), 73),\n",
       " (('l', 'v'), 72),\n",
       " (('h', 't'), 71),\n",
       " (('j', '<E>'), 71),\n",
       " (('x', 't'), 70),\n",
       " (('o', 'i'), 69),\n",
       " (('e', 'u'), 69),\n",
       " (('o', 'k'), 68),\n",
       " (('b', 'd'), 65),\n",
       " (('a', 'o'), 63),\n",
       " (('p', 'i'), 61),\n",
       " (('s', 'c'), 60),\n",
       " (('d', 'l'), 60),\n",
       " (('l', 'm'), 60),\n",
       " (('a', 'q'), 60),\n",
       " (('f', 'o'), 60),\n",
       " (('p', 'o'), 59),\n",
       " (('n', 'k'), 58),\n",
       " (('w', 'n'), 58),\n",
       " (('u', 'h'), 58),\n",
       " (('e', 'j'), 55),\n",
       " (('n', 'v'), 55),\n",
       " (('s', 'r'), 55),\n",
       " (('o', 'z'), 54),\n",
       " (('i', 'p'), 53),\n",
       " (('l', 'b'), 52),\n",
       " (('i', 'q'), 52),\n",
       " (('w', '<E>'), 51),\n",
       " (('m', 'c'), 51),\n",
       " (('s', 'p'), 51),\n",
       " (('e', 'w'), 50),\n",
       " (('k', 'u'), 50),\n",
       " (('v', 'r'), 48),\n",
       " (('u', 'g'), 47),\n",
       " (('o', 'x'), 45),\n",
       " (('u', 'z'), 45),\n",
       " (('z', 'z'), 45),\n",
       " (('j', 'h'), 45),\n",
       " (('b', 'u'), 45),\n",
       " (('o', 'g'), 44),\n",
       " (('n', 'r'), 44),\n",
       " (('f', 'f'), 44),\n",
       " (('n', 'j'), 44),\n",
       " (('z', 'h'), 43),\n",
       " (('c', 'c'), 42),\n",
       " (('r', 'b'), 41),\n",
       " (('x', 'o'), 41),\n",
       " (('b', 'h'), 41),\n",
       " (('p', 'p'), 39),\n",
       " (('x', 'l'), 39),\n",
       " (('h', 'v'), 39),\n",
       " (('b', 'b'), 38),\n",
       " (('m', 'p'), 38),\n",
       " (('x', 'x'), 38),\n",
       " (('u', 'v'), 37),\n",
       " (('x', 'e'), 36),\n",
       " (('w', 'o'), 36),\n",
       " (('c', 't'), 35),\n",
       " (('z', 'm'), 35),\n",
       " (('t', 's'), 35),\n",
       " (('m', 's'), 35),\n",
       " (('c', 'u'), 35),\n",
       " (('o', 'f'), 34),\n",
       " (('u', 'x'), 34),\n",
       " (('k', 'w'), 34),\n",
       " (('p', '<E>'), 33),\n",
       " (('g', 'l'), 32),\n",
       " (('z', 'r'), 32),\n",
       " (('d', 'n'), 31),\n",
       " (('g', 't'), 31),\n",
       " (('g', 'y'), 31),\n",
       " (('h', 's'), 31),\n",
       " (('x', 's'), 31),\n",
       " (('g', 's'), 30),\n",
       " (('x', 'y'), 30),\n",
       " (('y', 'g'), 30),\n",
       " (('d', 'm'), 30),\n",
       " (('d', 's'), 29),\n",
       " (('h', 'k'), 29),\n",
       " (('y', 'x'), 28),\n",
       " (('q', '<E>'), 28),\n",
       " (('g', 'n'), 27),\n",
       " (('y', 'b'), 27),\n",
       " (('g', 'w'), 26),\n",
       " (('n', 'h'), 26),\n",
       " (('k', 'n'), 26),\n",
       " (('g', 'g'), 25),\n",
       " (('d', 'g'), 25),\n",
       " (('l', 'c'), 25),\n",
       " (('r', 'j'), 25),\n",
       " (('w', 'u'), 25),\n",
       " (('l', 'k'), 24),\n",
       " (('m', 'd'), 24),\n",
       " (('s', 'w'), 24),\n",
       " (('s', 'n'), 24),\n",
       " (('h', 'd'), 24),\n",
       " (('w', 'h'), 23),\n",
       " (('y', 'j'), 23),\n",
       " (('y', 'y'), 23),\n",
       " (('r', 'z'), 23),\n",
       " (('d', 'w'), 23),\n",
       " (('w', 'r'), 22),\n",
       " (('t', 'n'), 22),\n",
       " (('l', 'f'), 22),\n",
       " (('y', 'h'), 22),\n",
       " (('r', 'w'), 21),\n",
       " (('s', 'b'), 21),\n",
       " (('m', 'n'), 20),\n",
       " (('f', 'l'), 20),\n",
       " (('w', 's'), 20),\n",
       " (('k', 'k'), 20),\n",
       " (('h', 'z'), 20),\n",
       " (('g', 'd'), 19),\n",
       " (('l', 'h'), 19),\n",
       " (('n', 'm'), 19),\n",
       " (('x', 'z'), 19),\n",
       " (('u', 'f'), 19),\n",
       " (('f', 't'), 18),\n",
       " (('l', 'r'), 18),\n",
       " (('p', 't'), 17),\n",
       " (('t', 'c'), 17),\n",
       " (('k', 't'), 17),\n",
       " (('d', 'v'), 17),\n",
       " (('u', 'p'), 16),\n",
       " (('p', 'l'), 16),\n",
       " (('l', 'w'), 16),\n",
       " (('p', 's'), 16),\n",
       " (('o', 'j'), 16),\n",
       " (('r', 'q'), 16),\n",
       " (('y', 'p'), 15),\n",
       " (('l', 'p'), 15),\n",
       " (('t', 'v'), 15),\n",
       " (('r', 'p'), 14),\n",
       " (('l', 'n'), 14),\n",
       " (('e', 'q'), 14),\n",
       " (('f', 'y'), 14),\n",
       " (('s', 'v'), 14),\n",
       " (('u', 'j'), 14),\n",
       " (('v', 'l'), 14),\n",
       " (('q', 'a'), 13),\n",
       " (('u', 'y'), 13),\n",
       " (('q', 'i'), 13),\n",
       " (('w', 'l'), 13),\n",
       " (('p', 'y'), 12),\n",
       " (('y', 'f'), 12),\n",
       " (('c', 'q'), 11),\n",
       " (('j', 'r'), 11),\n",
       " (('n', 'w'), 11),\n",
       " (('n', 'f'), 11),\n",
       " (('t', 'w'), 11),\n",
       " (('m', 'z'), 11),\n",
       " (('u', 'o'), 10),\n",
       " (('f', 'u'), 10),\n",
       " (('l', 'z'), 10),\n",
       " (('h', 'w'), 10),\n",
       " (('u', 'q'), 10),\n",
       " (('j', 'y'), 10),\n",
       " (('s', 'z'), 10),\n",
       " (('s', 'd'), 9),\n",
       " (('j', 'l'), 9),\n",
       " (('d', 'j'), 9),\n",
       " (('k', 'm'), 9),\n",
       " (('r', 'f'), 9),\n",
       " (('h', 'j'), 9),\n",
       " (('v', 'n'), 8),\n",
       " (('n', 'b'), 8),\n",
       " (('i', 'w'), 8),\n",
       " (('h', 'b'), 8),\n",
       " (('b', 's'), 8),\n",
       " (('w', 't'), 8),\n",
       " (('w', 'd'), 8),\n",
       " (('v', 'v'), 7),\n",
       " (('v', 'u'), 7),\n",
       " (('j', 's'), 7),\n",
       " (('m', 'j'), 7),\n",
       " (('f', 's'), 6),\n",
       " (('l', 'g'), 6),\n",
       " (('l', 'j'), 6),\n",
       " (('j', 'w'), 6),\n",
       " (('n', 'x'), 6),\n",
       " (('y', 'q'), 6),\n",
       " (('w', 'k'), 6),\n",
       " (('g', 'm'), 6),\n",
       " (('x', 'u'), 5),\n",
       " (('m', 'h'), 5),\n",
       " (('m', 'l'), 5),\n",
       " (('j', 'm'), 5),\n",
       " (('c', 's'), 5),\n",
       " (('j', 'v'), 5),\n",
       " (('n', 'p'), 5),\n",
       " (('d', 'f'), 5),\n",
       " (('x', 'd'), 5),\n",
       " (('z', 'b'), 4),\n",
       " (('f', 'n'), 4),\n",
       " (('x', 'c'), 4),\n",
       " (('m', 't'), 4),\n",
       " (('t', 'm'), 4),\n",
       " (('z', 'n'), 4),\n",
       " (('z', 't'), 4),\n",
       " (('p', 'u'), 4),\n",
       " (('c', 'z'), 4),\n",
       " (('b', 'n'), 4),\n",
       " (('z', 's'), 4),\n",
       " (('f', 'w'), 4),\n",
       " (('d', 't'), 4),\n",
       " (('j', 'd'), 4),\n",
       " (('j', 'c'), 4),\n",
       " (('y', 'w'), 4),\n",
       " (('v', 'k'), 3),\n",
       " (('x', 'w'), 3),\n",
       " (('t', 'j'), 3),\n",
       " (('c', 'j'), 3),\n",
       " (('q', 'w'), 3),\n",
       " (('g', 'b'), 3),\n",
       " (('o', 'q'), 3),\n",
       " (('r', 'x'), 3),\n",
       " (('d', 'c'), 3),\n",
       " (('g', 'j'), 3),\n",
       " (('x', 'f'), 3),\n",
       " (('z', 'w'), 3),\n",
       " (('d', 'k'), 3),\n",
       " (('u', 'u'), 3),\n",
       " (('m', 'v'), 3),\n",
       " (('c', 'x'), 3),\n",
       " (('l', 'q'), 3),\n",
       " (('p', 'b'), 2),\n",
       " (('t', 'g'), 2),\n",
       " (('q', 's'), 2),\n",
       " (('t', 'x'), 2),\n",
       " (('f', 'k'), 2),\n",
       " (('b', 't'), 2),\n",
       " (('j', 'n'), 2),\n",
       " (('k', 'c'), 2),\n",
       " (('z', 'k'), 2),\n",
       " (('s', 'j'), 2),\n",
       " (('s', 'f'), 2),\n",
       " (('z', 'j'), 2),\n",
       " (('n', 'q'), 2),\n",
       " (('f', 'z'), 2),\n",
       " (('h', 'g'), 2),\n",
       " (('w', 'w'), 2),\n",
       " (('k', 'j'), 2),\n",
       " (('j', 'k'), 2),\n",
       " (('w', 'm'), 2),\n",
       " (('z', 'c'), 2),\n",
       " (('z', 'v'), 2),\n",
       " (('w', 'f'), 2),\n",
       " (('q', 'm'), 2),\n",
       " (('k', 'z'), 2),\n",
       " (('j', 'j'), 2),\n",
       " (('z', 'p'), 2),\n",
       " (('j', 't'), 2),\n",
       " (('k', 'b'), 2),\n",
       " (('m', 'w'), 2),\n",
       " (('h', 'f'), 2),\n",
       " (('c', 'g'), 2),\n",
       " (('t', 'f'), 2),\n",
       " (('h', 'c'), 2),\n",
       " (('q', 'o'), 2),\n",
       " (('k', 'd'), 2),\n",
       " (('k', 'v'), 2),\n",
       " (('s', 'g'), 2),\n",
       " (('z', 'd'), 2),\n",
       " (('q', 'r'), 1),\n",
       " (('d', 'z'), 1),\n",
       " (('p', 'j'), 1),\n",
       " (('q', 'l'), 1),\n",
       " (('p', 'f'), 1),\n",
       " (('q', 'e'), 1),\n",
       " (('b', 'c'), 1),\n",
       " (('c', 'd'), 1),\n",
       " (('m', 'f'), 1),\n",
       " (('p', 'n'), 1),\n",
       " (('w', 'b'), 1),\n",
       " (('p', 'c'), 1),\n",
       " (('h', 'p'), 1),\n",
       " (('f', 'h'), 1),\n",
       " (('b', 'j'), 1),\n",
       " (('f', 'g'), 1),\n",
       " (('z', 'g'), 1),\n",
       " (('c', 'p'), 1),\n",
       " (('p', 'k'), 1),\n",
       " (('p', 'm'), 1),\n",
       " (('x', 'n'), 1),\n",
       " (('s', 'q'), 1),\n",
       " (('k', 'f'), 1),\n",
       " (('m', 'k'), 1),\n",
       " (('x', 'h'), 1),\n",
       " (('g', 'f'), 1),\n",
       " (('v', 'b'), 1),\n",
       " (('j', 'p'), 1),\n",
       " (('g', 'z'), 1),\n",
       " (('v', 'd'), 1),\n",
       " (('d', 'b'), 1),\n",
       " (('v', 'h'), 1),\n",
       " (('h', 'h'), 1),\n",
       " (('g', 'v'), 1),\n",
       " (('d', 'q'), 1),\n",
       " (('x', 'b'), 1),\n",
       " (('w', 'z'), 1),\n",
       " (('h', 'q'), 1),\n",
       " (('j', 'b'), 1),\n",
       " (('x', 'm'), 1),\n",
       " (('w', 'g'), 1),\n",
       " (('t', 'b'), 1),\n",
       " (('z', 'x'), 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(bigrams.items(), key = lambda kv:-kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build this model, we will generate a 26 x 26 matrix\n",
    "# each element will represent the probability that \n",
    "# the row value is preceded by the colum value\n",
    "# because we have two extra character <S> and <E>\n",
    "# we will actually build 28 x 28 matrix\n",
    "\n",
    "N = torch.zeros((28, 28), dtype = torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a sort of dictionary that can take a character\n",
    "# and turn it into the index we need for the matrix \n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "# a list of characters that appears in the words list\n",
    "stoi = {s:i for i, s in enumerate(chars)} \n",
    "# generates a dictionary with characters as key and index as value\n",
    "stoi['<S>'] = 26\n",
    "stoi['<E>'] = 27\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568, 2528,\n",
       "         1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,  182,\n",
       "         2050,  435,    0, 6640],\n",
       "        [ 321,   38,    1,   65,  655,    0,    0,   41,  217,    1,    0,  103,\n",
       "            0,    4,  105,    0,    0,  842,    8,    2,   45,    0,    0,    0,\n",
       "           83,    0,    0,  114],\n",
       "        [ 815,    0,   42,    1,  551,    0,    2,  664,  271,    3,  316,  116,\n",
       "            0,    0,  380,    1,   11,   76,    5,   35,   35,    0,    0,    3,\n",
       "          104,    4,    0,   97],\n",
       "        [1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,   60,\n",
       "           30,   31,  378,    0,    1,  424,   29,    4,   92,   17,   23,    0,\n",
       "          317,    1,    0,  516],\n",
       "        [ 679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178, 3248,\n",
       "          769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,  132,\n",
       "         1070,  181,    0, 3983],\n",
       "        [ 242,    0,    0,    0,  123,   44,    1,    1,  160,    0,    2,   20,\n",
       "            0,    4,   60,    0,    0,  114,    6,   18,   10,    0,    4,    0,\n",
       "           14,    2,    0,   80],\n",
       "        [ 330,    3,    0,   19,  334,    1,   25,  360,  190,    3,    0,   32,\n",
       "            6,   27,   83,    0,    0,  201,   30,   31,   85,    1,   26,    0,\n",
       "           31,    1,    0,  108],\n",
       "        [2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,  185,\n",
       "          117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,    0,\n",
       "          213,   20,    0, 2409],\n",
       "        [2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445, 1345,\n",
       "          427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,   89,\n",
       "          779,  277,    0, 2489],\n",
       "        [1473,    1,    4,    4,  440,    0,    0,   45,  119,    2,    2,    9,\n",
       "            5,    2,  479,    1,    0,   11,    7,    2,  202,    5,    6,    0,\n",
       "           10,    0,    0,   71],\n",
       "        [1731,    2,    2,    2,  895,    1,    0,  307,  509,    2,   20,  139,\n",
       "            9,   26,  344,    0,    0,  109,   95,   17,   50,    2,   34,    0,\n",
       "          379,    2,    0,  363],\n",
       "        [2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24, 1345,\n",
       "           60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,    0,\n",
       "         1588,   10,    0, 1314],\n",
       "        [2590,  112,   51,   24,  818,    1,    0,    5, 1256,    7,    1,    5,\n",
       "          168,   20,  452,   38,    0,   97,   35,    4,  139,    3,    2,    0,\n",
       "          287,   11,    0,  516],\n",
       "        [2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,  195,\n",
       "           19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,    6,\n",
       "          465,  145,    0, 6763],\n",
       "        [ 149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,  619,\n",
       "          261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,   45,\n",
       "          103,   54,    0,  855],\n",
       "        [ 209,    2,    1,    0,  197,    1,    0,  204,   61,    1,    1,   16,\n",
       "            1,    1,   59,   39,    0,  151,   16,   17,    4,    0,    0,    0,\n",
       "           12,    0,    0,   33],\n",
       "        [  13,    0,    0,    0,    1,    0,    0,    0,   13,    0,    0,    1,\n",
       "            2,    0,    2,    0,    0,    1,    2,    0,  206,    0,    3,    0,\n",
       "            0,    0,    0,   28],\n",
       "        [2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,  413,\n",
       "          162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,    3,\n",
       "          773,   23,    0, 1377],\n",
       "        [1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,  279,\n",
       "           90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,    0,\n",
       "          215,   10,    0, 1169],\n",
       "        [1027,    1,   17,    0,  716,    2,    2,  647,  532,    3,    0,  134,\n",
       "            4,   22,  667,    0,    0,  352,   35,  374,   78,   15,   11,    2,\n",
       "          341,  105,    0,  483],\n",
       "        [ 163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,  301,\n",
       "          154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,   34,\n",
       "           13,   45,    0,  155],\n",
       "        [ 642,    1,    0,    1,  568,    0,    0,    1,  911,    0,    3,   14,\n",
       "            0,    8,  153,    0,    0,   48,    0,    0,    7,    7,    0,    0,\n",
       "          121,    0,    0,   88],\n",
       "        [ 280,    1,    0,    8,  149,    2,    1,   23,  148,    0,    6,   13,\n",
       "            2,   58,   36,    0,    0,   22,   20,    8,   25,    0,    2,    0,\n",
       "           73,    1,    0,   51],\n",
       "        [ 103,    1,    4,    5,   36,    3,    0,    1,  102,    0,    0,   39,\n",
       "            1,    1,   41,    0,    0,    0,   31,   70,    5,    0,    3,   38,\n",
       "           30,   19,    0,  164],\n",
       "        [2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86, 1104,\n",
       "          148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,   28,\n",
       "           23,   78,    0, 2007],\n",
       "        [ 860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,  123,\n",
       "           35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,    1,\n",
       "          147,   45,    0,  160],\n",
       "        [4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963, 1572,\n",
       "         2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,  134,\n",
       "          535,  929,    0,    0],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for w in words:\n",
    "    chs = ['<S>'] + list(w) + ['<E>'] \n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        N[stoi[ch1], stoi[ch2]] += 1\n",
    "\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24e06887fd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {i:s for s, i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "\n",
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color=\"gray\")\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color=\"gray\")\n",
    "\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re shaping everything\n",
    "\n",
    "N = torch.zeros((27, 27), dtype = torch.int32)\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)} \n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.'] \n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        N[stoi[ch1], stoi[ch2]] += 1\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color=\"gray\")\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color=\"gray\")\n",
    "\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'z'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N[0]\n",
    "p = N[0].float()\n",
    "p = p / p.sum()\n",
    "p\n",
    "g = torch.Generator().manual_seed(69696929)\n",
    "itos[torch.multinomial(p, 1, replacement=True, generator=g).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prob_dist = N.float()\n",
    "Prob_dist /= Prob_dist.sum(1, keepdim=True)\n",
    "\n",
    "for i in range(27):\n",
    "    print (Prob_dist[i].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a name predictor that just prints bunch of new name\n",
    "\n",
    "g = torch.Generator().manual_seed(1093)\n",
    "for i in range(10):\n",
    "    index = 0\n",
    "    name = []\n",
    "    while True:\n",
    "        p = Prob_dist[index]\n",
    "        predicted_index = torch.multinomial(p, 1, replacement=True, generator=g).item()\n",
    "        if predicted_index == 0:\n",
    "            break\n",
    "        name.append(itos[predicted_index])\n",
    "        index = predicted_index\n",
    "    print(''.join(name))\n",
    "\n",
    "# weird lists... \n",
    "# bigram is terrible..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_log_likelihood/count=2.454094088872915\n"
     ]
    }
   ],
   "source": [
    "# we need to somehow figure out the probability of the model\n",
    "# evaluate quality of the model using a number\n",
    "\n",
    "log_prob = 0\n",
    "count = 0\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.'] \n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        prob = Prob_dist[stoi[ch1], stoi[ch2]]\n",
    "        count += 1\n",
    "        log_prob += torch.log(prob)\n",
    "\n",
    "negative_log_likelihood = -log_prob.item()\n",
    "print(f\"{negative_log_likelihood/count=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal... our model is shit...\n",
    "# We did train our model, but the likelihood of this values are horrible\n",
    "# Note on likelihood:\n",
    "# so our model has 27 x 27 probability value stored in a matrix Prob_dist\n",
    "# and in our sample we have a value .emma.\n",
    "# probability of that word should be large, right? because it is in the sample... \n",
    "# so our distribution should such that, probability of having .emma. should be large\n",
    "# that probability value is = Pr[.e]xPr[em]xPr[ma]xPr[a.]\n",
    "# So probability of the entire sample is multiple of such probability of words in the sample\n",
    "# ... that means we need to multiply all the probability.. which is painful... and the value that it returns will be extremely small\n",
    "# a good solution, use log....\n",
    "# so the thing we want to maximize = sum log(prob(word))\n",
    "# loss function should be opposite of it... so loss = -sum log(prob(word))\n",
    "# for better scaling we use -sum log(prob(word)) / sample_size.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current problem... if some prob is 0... log will straight out return inf\n",
    "# solution: have all value have 1 initially.. to smooth out\n",
    "\n",
    "Prob_dist = (N+1).float()\n",
    "Prob_dist /= Prob_dist.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_log_likelihood/count=2.4543562565199477\n"
     ]
    }
   ],
   "source": [
    "log_prob = 0\n",
    "count = 0\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.'] \n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        prob = Prob_dist[stoi[ch1], stoi[ch2]]\n",
    "        count += 1\n",
    "        log_prob += torch.log(prob)\n",
    "\n",
    "negative_log_likelihood = -log_prob.item()\n",
    "print(f\"{negative_log_likelihood/count=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_log_likelihood/count=2.479089895884196\n"
     ]
    }
   ],
   "source": [
    "# so let's calculate likelihood of one word: shamit\n",
    "\n",
    "log_prob = 0\n",
    "count = 0\n",
    "my_word = 'shamit'\n",
    "for w in my_word:\n",
    "    chs = ['.'] + list(w) + ['.'] \n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        prob = Prob_dist[stoi[ch1], stoi[ch2]]\n",
    "        count += 1\n",
    "        log_prob += torch.log(prob)\n",
    "\n",
    "negative_log_likelihood = -log_prob.item()\n",
    "print(f\"{negative_log_likelihood/count=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use neural network ^_^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training set\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:3]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs , chs[1:]):\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to feed the xs and ys into neural network\n",
    "# but directly giving it into neural network isn't advisable... \n",
    "# instead, use on_hot approach\n",
    "# generate a vector [0 0 0 ... 0] (27 len)\n",
    "# for xs = 13, only 1 the 13th position.. UwU\n",
    "# pytorch already have this though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's go with the implementation\n",
    "import torch.nn.functional as torchFunc\n",
    "# forward pass:\n",
    "\n",
    "# get input x outputs\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs , chs[1:]):\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "\n",
    "# generate initial weight matrix for all 27 neuron... \n",
    "# each neuron has 27 input... so matrix -> 27 x 27\n",
    "g = torch.Generator().manual_seed(69696969)\n",
    "W = torch.randn((27, 27), dtype=torch.float, generator=g, requires_grad=True)\n",
    "\n",
    "# encoding xs values to be proper input\n",
    "x_enc = torchFunc.one_hot(xs, num_classes=27).float()\n",
    "logits = x_enc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "# this two functions are often called softmax\n",
    "# e^z / sum (e^z)\n",
    "\n",
    "# need to count loss\n",
    "loss = -probs[torch.arange(5), ys].log().mean()\n",
    "\n",
    "# generate backward pass\n",
    "W.grad = None\n",
    "loss.backward()\n",
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)} \n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "import torch.nn.functional as torchFunc\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 27]) torch.Size([228146, 27])\n"
     ]
    }
   ],
   "source": [
    "# g = torch.Generator().manual_seed(10)\n",
    "# Weights = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "# Bias = torch.randn((1, 27), generator=g, requires_grad=True)\n",
    "# logits = x_enc @ Weights + Bias\n",
    "# counts = logits.exp()\n",
    "# prob = counts / counts.sum(1, keepdim=True)\n",
    "# print(prob.shape, x_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.861180067062378\n",
      "3.3933422565460205\n",
      "3.1596736907958984\n",
      "3.0209739208221436\n",
      "2.928812026977539\n",
      "2.8608152866363525\n",
      "2.8090059757232666\n",
      "2.7685110569000244\n",
      "2.7362160682678223\n",
      "2.7100257873535156\n",
      "2.6884427070617676\n",
      "2.6703739166259766\n",
      "2.6550216674804688\n",
      "2.64180588722229\n",
      "2.6302995681762695\n",
      "2.6201844215393066\n",
      "2.6112172603607178\n",
      "2.6032090187072754\n",
      "2.596011161804199\n",
      "2.589503765106201\n",
      "2.5835890769958496\n",
      "2.5781874656677246\n",
      "2.573232650756836\n",
      "2.5686700344085693\n",
      "2.5644524097442627\n",
      "2.5605406761169434\n",
      "2.5569019317626953\n",
      "2.5535082817077637\n",
      "2.5503346920013428\n",
      "2.547360420227051\n",
      "2.544567823410034\n",
      "2.5419414043426514\n",
      "2.5394668579101562\n",
      "2.5371322631835938\n",
      "2.5349268913269043\n",
      "2.532841444015503\n",
      "2.530867338180542\n",
      "2.528996706008911\n",
      "2.5272223949432373\n",
      "2.525538921356201\n",
      "2.523939371109009\n",
      "2.522418737411499\n",
      "2.520972490310669\n",
      "2.5195956230163574\n",
      "2.518284797668457\n",
      "2.5170347690582275\n",
      "2.515842914581299\n",
      "2.5147058963775635\n",
      "2.513620376586914\n",
      "2.5125834941864014\n",
      "2.511592149734497\n",
      "2.5106444358825684\n",
      "2.509737491607666\n",
      "2.508869171142578\n",
      "2.50803804397583\n",
      "2.5072407722473145\n",
      "2.506477117538452\n",
      "2.505744218826294\n",
      "2.5050408840179443\n",
      "2.5043654441833496\n",
      "2.5037171840667725\n",
      "2.503093719482422\n",
      "2.5024940967559814\n",
      "2.501917600631714\n",
      "2.5013628005981445\n",
      "2.500828266143799\n",
      "2.5003132820129395\n",
      "2.499817371368408\n",
      "2.4993391036987305\n",
      "2.498877763748169\n",
      "2.4984323978424072\n",
      "2.498002767562866\n",
      "2.4975874423980713\n",
      "2.4971861839294434\n",
      "2.4967987537384033\n",
      "2.4964241981506348\n",
      "2.496061086654663\n",
      "2.495710611343384\n",
      "2.495370626449585\n",
      "2.495041847229004\n",
      "2.494723081588745\n",
      "2.4944145679473877\n",
      "2.494115114212036\n",
      "2.4938251972198486\n",
      "2.493543863296509\n",
      "2.4932708740234375\n",
      "2.4930057525634766\n",
      "2.492748260498047\n",
      "2.4924983978271484\n",
      "2.492255687713623\n",
      "2.4920201301574707\n",
      "2.491790771484375\n",
      "2.4915683269500732\n",
      "2.491352081298828\n",
      "2.4911413192749023\n",
      "2.490936756134033\n",
      "2.490737199783325\n",
      "2.4905433654785156\n",
      "2.490354299545288\n",
      "2.490170478820801\n",
      "2.4899916648864746\n",
      "2.4898171424865723\n",
      "2.489647150039673\n",
      "2.4894814491271973\n",
      "2.4893202781677246\n",
      "2.4891626834869385\n",
      "2.489009141921997\n",
      "2.4888594150543213\n",
      "2.4887137413024902\n",
      "2.4885714054107666\n",
      "2.4884324073791504\n",
      "2.4882969856262207\n",
      "2.4881644248962402\n",
      "2.4880354404449463\n",
      "2.4879090785980225\n",
      "2.487785816192627\n",
      "2.4876654148101807\n",
      "2.4875481128692627\n",
      "2.4874331951141357\n",
      "2.4873206615448\n",
      "2.487211227416992\n",
      "2.4871037006378174\n",
      "2.4869987964630127\n",
      "2.486896276473999\n",
      "2.4867961406707764\n",
      "2.4866979122161865\n",
      "2.4866018295288086\n",
      "2.4865078926086426\n",
      "2.4864156246185303\n",
      "2.486325740814209\n",
      "2.4862377643585205\n",
      "2.4861512184143066\n",
      "2.4860668182373047\n",
      "2.4859843254089355\n",
      "2.485902786254883\n",
      "2.485823392868042\n",
      "2.485745906829834\n",
      "2.4856698513031006\n",
      "2.4855949878692627\n",
      "2.4855215549468994\n",
      "2.485450029373169\n",
      "2.485379695892334\n",
      "2.4853105545043945\n",
      "2.4852428436279297\n",
      "2.4851765632629395\n",
      "2.485111713409424\n",
      "2.4850475788116455\n",
      "2.484984874725342\n",
      "2.4849233627319336\n",
      "2.484862804412842\n",
      "2.484804153442383\n",
      "2.484745979309082\n",
      "2.4846889972686768\n",
      "2.484632730484009\n",
      "2.4845778942108154\n",
      "2.4845237731933594\n",
      "2.484471082687378\n",
      "2.4844188690185547\n",
      "2.484367609024048\n",
      "2.4843175411224365\n",
      "2.4842677116394043\n",
      "2.484219551086426\n",
      "2.4841718673706055\n",
      "2.4841248989105225\n",
      "2.484078884124756\n",
      "2.4840335845947266\n",
      "2.4839892387390137\n",
      "2.483945369720459\n",
      "2.4839024543762207\n",
      "2.4838600158691406\n",
      "2.483818531036377\n",
      "2.4837775230407715\n",
      "2.4837377071380615\n",
      "2.4836976528167725\n",
      "2.483659029006958\n",
      "2.4836206436157227\n",
      "2.4835827350616455\n",
      "2.4835457801818848\n",
      "2.4835093021392822\n",
      "2.483473300933838\n",
      "2.48343825340271\n",
      "2.483403205871582\n",
      "2.4833691120147705\n",
      "2.483335256576538\n",
      "2.483302354812622\n",
      "2.483269691467285\n",
      "2.4832372665405273\n",
      "2.483205795288086\n",
      "2.4831748008728027\n",
      "2.4831435680389404\n",
      "2.4831135272979736\n",
      "2.483083724975586\n",
      "2.4830546379089355\n",
      "2.483025550842285\n",
      "2.482996702194214\n",
      "2.482968807220459\n",
      "2.482941150665283\n",
      "2.4829139709472656\n",
      "2.4828872680664062\n",
      "2.482860565185547\n",
      "2.482834577560425\n",
      "2.482808828353882\n",
      "2.482783555984497\n",
      "2.4827582836151123\n",
      "2.482733964920044\n",
      "2.4827094078063965\n",
      "2.4826858043670654\n",
      "2.4826619625091553\n",
      "2.4826388359069824\n",
      "2.4826157093048096\n",
      "2.482593059539795\n",
      "2.4825704097747803\n",
      "2.482548713684082\n",
      "2.482527256011963\n",
      "2.4825057983398438\n",
      "2.4824843406677246\n",
      "2.4824635982513428\n",
      "2.482442855834961\n",
      "2.4824228286743164\n",
      "2.4824025630950928\n",
      "2.4823830127716064\n",
      "2.48236346244812\n",
      "2.482344388961792\n",
      "2.482325315475464\n",
      "2.482306480407715\n",
      "2.482288122177124\n",
      "2.482269525527954\n",
      "2.4822516441345215\n",
      "2.482234001159668\n",
      "2.4822163581848145\n",
      "2.48219895362854\n",
      "2.482182264328003\n",
      "2.4821650981903076\n",
      "2.4821484088897705\n",
      "2.4821321964263916\n",
      "2.4821155071258545\n",
      "2.482099771499634\n",
      "2.482083797454834\n",
      "2.4820680618286133\n",
      "2.4820525646209717\n",
      "2.482037305831909\n",
      "2.482022523880005\n",
      "2.4820075035095215\n",
      "2.481992721557617\n",
      "2.481978178024292\n",
      "2.481963872909546\n",
      "2.4819495677948\n",
      "2.481935501098633\n",
      "2.481921911239624\n",
      "2.481908082962036\n",
      "2.4818942546844482\n",
      "2.4818811416625977\n",
      "2.481868028640747\n",
      "2.4818549156188965\n",
      "2.481842041015625\n",
      "2.4818294048309326\n",
      "2.4818167686462402\n",
      "2.481804370880127\n",
      "2.4817917346954346\n",
      "2.4817798137664795\n",
      "2.4817678928375244\n",
      "2.4817559719085693\n",
      "2.4817440509796143\n",
      "2.4817323684692383\n",
      "2.4817206859588623\n",
      "2.4817097187042236\n",
      "2.481698513031006\n",
      "2.481687545776367\n",
      "2.4816763401031494\n",
      "2.48166561126709\n",
      "2.4816548824310303\n",
      "2.48164439201355\n",
      "2.4816336631774902\n",
      "2.481623411178589\n",
      "2.4816131591796875\n",
      "2.481602907180786\n",
      "2.481593132019043\n",
      "2.4815831184387207\n",
      "2.4815731048583984\n",
      "2.4815635681152344\n",
      "2.4815540313720703\n",
      "2.4815444946289062\n",
      "2.4815354347229004\n",
      "2.4815258979797363\n",
      "2.4815165996551514\n",
      "2.4815077781677246\n",
      "2.4814987182617188\n",
      "2.481489419937134\n",
      "2.481480836868286\n",
      "2.4814724922180176\n",
      "2.481463670730591\n",
      "2.4814553260803223\n",
      "2.4814467430114746\n",
      "2.481438398361206\n",
      "2.4814298152923584\n",
      "2.481421709060669\n",
      "2.4814136028289795\n",
      "2.4814059734344482\n",
      "2.4813976287841797\n",
      "2.4813899993896484\n",
      "2.481382131576538\n",
      "2.481374502182007\n",
      "2.4813663959503174\n",
      "2.4813592433929443\n",
      "2.4813520908355713\n",
      "2.48134446144104\n",
      "2.481337308883667\n",
      "2.4813296794891357\n",
      "2.481322765350342\n",
      "2.4813156127929688\n",
      "2.4813084602355957\n",
      "2.481301784515381\n",
      "2.481294870376587\n",
      "2.481288194656372\n",
      "2.4812815189361572\n",
      "2.4812748432159424\n",
      "2.4812679290771484\n",
      "2.4812612533569336\n",
      "2.481255054473877\n",
      "2.481248378753662\n",
      "2.4812421798706055\n",
      "2.4812357425689697\n",
      "2.481229543685913\n",
      "2.4812235832214355\n",
      "2.4812171459198\n",
      "2.4812111854553223\n",
      "2.481205463409424\n",
      "2.4811997413635254\n",
      "2.4811935424804688\n",
      "2.481187582015991\n",
      "2.4811818599700928\n",
      "2.4811761379241943\n",
      "2.481170654296875\n",
      "2.4811646938323975\n",
      "2.481159210205078\n",
      "2.481153964996338\n",
      "2.4811482429504395\n",
      "2.481142997741699\n",
      "2.481137752532959\n",
      "2.481132745742798\n",
      "2.4811272621154785\n",
      "2.4811220169067383\n",
      "2.481116771697998\n",
      "2.481112003326416\n",
      "2.481106758117676\n",
      "2.4811015129089355\n",
      "2.4810965061187744\n",
      "2.4810914993286133\n",
      "2.4810869693756104\n",
      "2.4810822010040283\n",
      "2.481077194213867\n",
      "2.481072425842285\n",
      "2.481067657470703\n",
      "2.4810631275177\n",
      "2.481058359146118\n",
      "2.4810540676116943\n",
      "2.4810495376586914\n",
      "2.4810447692871094\n",
      "2.4810400009155273\n",
      "2.4810359477996826\n",
      "2.481031656265259\n",
      "2.481027364730835\n",
      "2.481022834777832\n",
      "2.4810187816619873\n",
      "2.4810142517089844\n",
      "2.4810099601745605\n",
      "2.481005907058716\n",
      "2.481001615524292\n",
      "2.4809978008270264\n",
      "2.4809939861297607\n",
      "2.480989694595337\n",
      "2.4809858798980713\n",
      "2.4809818267822266\n",
      "2.480977773666382\n",
      "2.4809741973876953\n",
      "2.4809701442718506\n",
      "2.480966091156006\n",
      "2.4809627532958984\n",
      "2.480958938598633\n",
      "2.480954885482788\n",
      "2.4809515476226807\n",
      "2.480947732925415\n",
      "2.4809441566467285\n",
      "2.480940580368042\n",
      "2.4809370040893555\n",
      "2.480933666229248\n",
      "2.4809298515319824\n",
      "2.480926275253296\n",
      "2.4809231758117676\n",
      "2.48091983795166\n",
      "2.4809162616729736\n",
      "2.480912923812866\n",
      "2.480909585952759\n",
      "2.4809064865112305\n",
      "2.480903148651123\n",
      "2.4808993339538574\n",
      "2.4808967113494873\n",
      "2.480893611907959\n",
      "2.4808897972106934\n",
      "2.4808871746063232\n",
      "2.480883836746216\n",
      "2.4808807373046875\n",
      "2.480877637863159\n",
      "2.480874538421631\n",
      "2.4808716773986816\n",
      "2.480868339538574\n",
      "2.480865716934204\n",
      "2.480862617492676\n",
      "2.4808595180511475\n",
      "2.4808566570281982\n",
      "2.480854034423828\n",
      "2.4808509349823\n",
      "2.4808480739593506\n",
      "2.4808454513549805\n",
      "2.4808425903320312\n",
      "2.480839967727661\n",
      "2.480837345123291\n",
      "2.4808340072631836\n",
      "2.4808316230773926\n",
      "2.4808292388916016\n",
      "2.4808261394500732\n",
      "2.4808237552642822\n",
      "2.480820894241333\n",
      "2.480818510055542\n",
      "2.480815887451172\n",
      "2.4808132648468018\n",
      "2.4808108806610107\n",
      "2.4808082580566406\n",
      "2.4808058738708496\n",
      "2.4808034896850586\n",
      "2.4808008670806885\n",
      "2.4807982444763184\n",
      "2.4807960987091064\n",
      "2.4807937145233154\n",
      "2.480790853500366\n",
      "2.480788469314575\n",
      "2.4807863235473633\n",
      "2.4807839393615723\n",
      "2.4807817935943604\n",
      "2.4807794094085693\n",
      "2.4807770252227783\n",
      "2.4807751178741455\n",
      "2.4807724952697754\n",
      "2.4807703495025635\n",
      "2.4807682037353516\n",
      "2.4807660579681396\n",
      "2.4807636737823486\n",
      "2.480761766433716\n",
      "2.480759382247925\n",
      "2.480757474899292\n",
      "2.48075532913208\n",
      "2.480752944946289\n",
      "2.4807510375976562\n",
      "2.4807488918304443\n",
      "2.4807469844818115\n",
      "2.4807446002960205\n",
      "2.4807426929473877\n",
      "2.480740785598755\n",
      "2.480739116668701\n",
      "2.48073673248291\n",
      "2.4807350635528564\n",
      "2.4807329177856445\n",
      "2.4807307720184326\n",
      "2.480729103088379\n",
      "2.480726957321167\n",
      "2.4807252883911133\n",
      "2.4807233810424805\n",
      "2.4807214736938477\n",
      "2.480719566345215\n",
      "2.480717897415161\n",
      "2.4807159900665283\n",
      "2.4807143211364746\n",
      "2.480712413787842\n",
      "2.480710506439209\n",
      "2.480708599090576\n",
      "2.4807069301605225\n",
      "2.4807047843933105\n",
      "2.480703353881836\n",
      "2.4807016849517822\n",
      "2.4806997776031494\n",
      "2.4806981086730957\n",
      "2.4806969165802\n",
      "2.4806947708129883\n",
      "2.4806931018829346\n",
      "2.480691432952881\n",
      "2.4806900024414062\n",
      "2.4806883335113525\n",
      "2.480686902999878\n",
      "2.480685234069824\n",
      "2.4806833267211914\n",
      "2.4806816577911377\n",
      "2.480680227279663\n",
      "2.4806787967681885\n",
      "2.480677366256714\n",
      "2.480675458908081\n",
      "2.4806740283966064\n",
      "2.480672836303711\n",
      "2.480670928955078\n",
      "2.4806694984436035\n",
      "2.48066782951355\n",
      "2.4806666374206543\n",
      "2.4806652069091797\n",
      "2.480663537979126\n",
      "2.4806621074676514\n",
      "2.4806606769561768\n",
      "2.480659246444702\n",
      "2.4806575775146484\n",
      "2.480656623840332\n",
      "2.4806551933288574\n",
      "2.4806535243988037\n",
      "2.480652093887329\n",
      "2.4806506633758545\n",
      "2.480649471282959\n",
      "2.4806480407714844\n",
      "2.4806466102600098\n",
      "2.4806456565856934\n",
      "2.4806439876556396\n",
      "2.480642795562744\n",
      "2.4806413650512695\n",
      "2.480640172958374\n",
      "2.4806389808654785\n",
      "2.480637550354004\n",
      "2.4806363582611084\n",
      "2.4806346893310547\n",
      "2.4806337356567383\n",
      "2.4806320667266846\n",
      "2.4806315898895264\n",
      "2.4806299209594727\n",
      "2.4806289672851562\n",
      "2.4806272983551025\n",
      "2.480626344680786\n",
      "2.4806251525878906\n",
      "2.480623722076416\n",
      "2.4806225299835205\n",
      "2.480621576309204\n",
      "2.4806206226348877\n",
      "2.480619192123413\n",
      "2.4806182384490967\n",
      "2.480617046356201\n",
      "2.4806158542633057\n",
      "2.48061466217041\n",
      "2.4806134700775146\n",
      "2.4806125164031982\n",
      "2.4806108474731445\n",
      "2.480609893798828\n",
      "2.4806089401245117\n",
      "2.480607748031616\n",
      "2.480607032775879\n",
      "2.4806056022644043\n",
      "2.480604887008667\n",
      "2.4806036949157715\n",
      "2.480602264404297\n",
      "2.4806015491485596\n",
      "2.480600357055664\n",
      "2.4805994033813477\n",
      "2.480598211288452\n",
      "2.4805972576141357\n",
      "2.4805965423583984\n",
      "2.480595111846924\n",
      "2.4805943965911865\n",
      "2.480593204498291\n",
      "2.4805924892425537\n",
      "2.480591297149658\n",
      "2.480590581893921\n",
      "2.4805893898010254\n",
      "2.480588436126709\n",
      "2.4805874824523926\n",
      "2.480586290359497\n",
      "2.4805855751037598\n",
      "2.4805846214294434\n",
      "2.480583429336548\n",
      "2.4805829524993896\n",
      "2.480581521987915\n",
      "2.4805808067321777\n",
      "2.4805800914764404\n",
      "2.480578899383545\n",
      "2.4805781841278076\n",
      "2.480576992034912\n",
      "2.4805760383605957\n",
      "2.4805755615234375\n",
      "2.480574607849121\n",
      "2.4805734157562256\n",
      "2.4805727005004883\n",
      "2.480571985244751\n",
      "2.4805707931518555\n",
      "2.480570077896118\n",
      "2.480569362640381\n",
      "2.4805684089660645\n",
      "2.480567455291748\n",
      "2.4805665016174316\n",
      "2.4805655479431152\n",
      "2.480565071105957\n",
      "2.4805643558502197\n",
      "2.4805634021759033\n",
      "2.480562686920166\n",
      "2.4805619716644287\n",
      "2.4805612564086914\n",
      "2.480560302734375\n",
      "2.4805591106414795\n",
      "2.4805586338043213\n",
      "2.480558156967163\n",
      "2.4805569648742676\n",
      "2.4805562496185303\n",
      "2.480555534362793\n",
      "2.4805545806884766\n",
      "2.4805538654327393\n",
      "2.480553150177002\n",
      "2.4805524349212646\n",
      "2.4805517196655273\n",
      "2.48055100440979\n",
      "2.4805502891540527\n",
      "2.4805495738983154\n",
      "2.480548620223999\n",
      "2.48054838180542\n",
      "2.4805471897125244\n",
      "2.480546236038208\n",
      "2.480546236038208\n",
      "2.4805452823638916\n",
      "2.480544328689575\n",
      "2.480543851852417\n",
      "2.4805428981781006\n",
      "2.4805421829223633\n",
      "2.480541706085205\n",
      "2.480541229248047\n",
      "2.4805405139923096\n",
      "2.4805397987365723\n",
      "2.480539083480835\n",
      "2.4805381298065186\n",
      "2.4805376529693604\n",
      "2.480536937713623\n",
      "2.480536460876465\n",
      "2.4805357456207275\n",
      "2.4805350303649902\n",
      "2.480534553527832\n",
      "2.4805335998535156\n",
      "2.4805331230163574\n",
      "2.48053240776062\n",
      "2.480531930923462\n",
      "2.4805309772491455\n",
      "2.4805305004119873\n",
      "2.48052978515625\n",
      "2.480529308319092\n",
      "2.4805290699005127\n",
      "2.4805283546447754\n",
      "2.480527639389038\n",
      "2.48052716255188\n",
      "2.4805264472961426\n",
      "2.4805257320404053\n",
      "2.480525255203247\n",
      "2.4805245399475098\n",
      "2.4805238246917725\n",
      "2.4805233478546143\n",
      "2.480522632598877\n",
      "2.4805221557617188\n",
      "2.4805216789245605\n",
      "2.4805209636688232\n",
      "2.480520486831665\n",
      "2.4805197715759277\n",
      "2.4805192947387695\n",
      "2.4805188179016113\n",
      "2.480518341064453\n",
      "2.480517625808716\n",
      "2.4805173873901367\n",
      "2.4805164337158203\n",
      "2.480515956878662\n",
      "2.480515956878662\n",
      "2.480515241622925\n",
      "2.4805145263671875\n",
      "2.4805140495300293\n",
      "2.480513334274292\n",
      "2.480512857437134\n",
      "2.4805126190185547\n",
      "2.4805121421813965\n",
      "2.48051118850708\n",
      "2.480510950088501\n",
      "2.4805104732513428\n",
      "2.4805099964141846\n",
      "2.4805092811584473\n",
      "2.480508804321289\n",
      "2.4805080890655518\n",
      "2.4805078506469727\n",
      "2.4805073738098145\n",
      "2.4805068969726562\n",
      "2.480506181716919\n",
      "2.48050594329834\n",
      "2.4805052280426025\n",
      "2.4805047512054443\n",
      "2.480504274368286\n",
      "2.480504035949707\n",
      "2.480503559112549\n",
      "2.4805028438568115\n",
      "2.4805026054382324\n",
      "2.480502128601074\n",
      "2.480501651763916\n",
      "2.480501174926758\n",
      "2.4805006980895996\n",
      "2.4805004596710205\n",
      "2.4804999828338623\n",
      "2.480499505996704\n",
      "2.480498790740967\n",
      "2.4804983139038086\n",
      "2.4804980754852295\n",
      "2.4804978370666504\n",
      "2.480497121810913\n",
      "2.480496883392334\n",
      "2.4804961681365967\n",
      "2.4804956912994385\n",
      "2.4804954528808594\n",
      "2.480494976043701\n",
      "2.480494737625122\n",
      "2.480494260787964\n",
      "2.4804935455322266\n",
      "2.4804930686950684\n",
      "2.4804930686950684\n",
      "2.480492353439331\n",
      "2.480491876602173\n",
      "2.4804916381835938\n",
      "2.4804911613464355\n",
      "2.4804904460906982\n",
      "2.4804904460906982\n",
      "2.48048996925354\n",
      "2.48048996925354\n",
      "2.4804890155792236\n",
      "2.4804887771606445\n",
      "2.4804885387420654\n",
      "2.4804880619049072\n",
      "2.480487823486328\n",
      "2.48048734664917\n",
      "2.48048734664917\n",
      "2.4804866313934326\n",
      "2.4804861545562744\n",
      "2.4804859161376953\n",
      "2.480485200881958\n",
      "2.4804847240448\n",
      "2.4804844856262207\n",
      "2.4804840087890625\n",
      "2.4804840087890625\n",
      "2.4804835319519043\n",
      "2.480483293533325\n",
      "2.480482578277588\n",
      "2.4804821014404297\n",
      "2.4804821014404297\n",
      "2.4804821014404297\n",
      "2.4804816246032715\n",
      "2.480480670928955\n",
      "2.480480670928955\n",
      "2.4804799556732178\n",
      "2.480480194091797\n",
      "2.4804797172546387\n",
      "2.4804792404174805\n",
      "2.4804790019989014\n",
      "2.480478286743164\n",
      "2.480478286743164\n",
      "2.480478048324585\n",
      "2.480477809906006\n",
      "2.4804770946502686\n",
      "2.4804768562316895\n",
      "2.4804766178131104\n",
      "2.480476140975952\n",
      "2.480475902557373\n",
      "2.480475425720215\n",
      "2.4804751873016357\n",
      "2.4804749488830566\n",
      "2.4804749488830566\n",
      "2.4804742336273193\n",
      "2.4804739952087402\n",
      "2.480473756790161\n",
      "2.480473279953003\n",
      "2.480473041534424\n",
      "2.480473041534424\n",
      "2.4804725646972656\n",
      "2.4804718494415283\n",
      "2.4804718494415283\n",
      "2.480471134185791\n",
      "2.48047137260437\n",
      "2.480471134185791\n",
      "2.4804704189300537\n",
      "2.4804704189300537\n",
      "2.4804699420928955\n",
      "2.4804697036743164\n",
      "2.4804694652557373\n",
      "2.480468988418579\n",
      "2.48046875\n",
      "2.480468511581421\n",
      "2.480468511581421\n",
      "2.4804680347442627\n",
      "2.4804677963256836\n",
      "2.4804675579071045\n",
      "2.4804673194885254\n",
      "2.480466842651367\n",
      "2.480466604232788\n",
      "2.48046612739563\n",
      "2.480465888977051\n",
      "2.480465888977051\n",
      "2.4804654121398926\n",
      "2.4804651737213135\n",
      "2.4804649353027344\n",
      "2.4804646968841553\n",
      "2.480464220046997\n",
      "2.480464220046997\n",
      "2.480463981628418\n",
      "2.480463743209839\n",
      "2.4804635047912598\n",
      "2.4804630279541016\n",
      "2.4804630279541016\n",
      "2.4804627895355225\n",
      "2.4804623126983643\n",
      "2.480462074279785\n",
      "2.480461835861206\n",
      "2.480461597442627\n",
      "2.480461359024048\n",
      "2.4804608821868896\n",
      "2.4804608821868896\n",
      "2.4804604053497314\n",
      "2.4804604053497314\n",
      "2.4804601669311523\n",
      "2.4804599285125732\n",
      "2.480459451675415\n",
      "2.480459451675415\n",
      "2.480458974838257\n",
      "2.4804587364196777\n",
      "2.4804584980010986\n",
      "2.4804582595825195\n",
      "2.4804582595825195\n",
      "2.4804577827453613\n",
      "2.4804577827453613\n",
      "2.480457067489624\n",
      "2.480456829071045\n",
      "2.480457067489624\n",
      "2.480456590652466\n",
      "2.4804563522338867\n",
      "2.4804561138153076\n",
      "2.4804558753967285\n",
      "2.4804558753967285\n",
      "2.4804556369781494\n",
      "2.4804553985595703\n",
      "2.480455160140991\n",
      "2.480454921722412\n",
      "2.480454444885254\n",
      "2.480454444885254\n",
      "2.4804539680480957\n",
      "2.4804539680480957\n",
      "2.4804539680480957\n",
      "2.4804530143737793\n",
      "2.4804534912109375\n",
      "2.4804530143737793\n",
      "2.4804530143737793\n",
      "2.480452537536621\n",
      "2.480452299118042\n",
      "2.480452060699463\n",
      "2.480452060699463\n",
      "2.480452060699463\n",
      "2.480451822280884\n",
      "2.4804513454437256\n",
      "2.4804513454437256\n",
      "2.4804511070251465\n",
      "2.4804506301879883\n",
      "2.480450391769409\n",
      "2.4804506301879883\n",
      "2.48045015335083\n",
      "2.480449914932251\n",
      "2.480449914932251\n",
      "2.4804494380950928\n",
      "2.480449676513672\n",
      "2.4804491996765137\n",
      "2.4804489612579346\n",
      "2.4804487228393555\n",
      "2.4804487228393555\n",
      "2.4804484844207764\n",
      "2.4804482460021973\n",
      "2.480448007583618\n",
      "2.480448007583618\n",
      "2.48044753074646\n",
      "2.48044753074646\n",
      "2.480447292327881\n",
      "2.4804468154907227\n",
      "2.4804468154907227\n",
      "2.4804468154907227\n",
      "2.4804465770721436\n",
      "2.4804463386535645\n",
      "2.4804463386535645\n",
      "2.4804458618164062\n",
      "2.4804461002349854\n",
      "2.480445623397827\n",
      "2.480445623397827\n",
      "2.480445384979248\n",
      "2.4804446697235107\n",
      "2.48044490814209\n",
      "2.4804446697235107\n",
      "2.4804444313049316\n",
      "2.4804444313049316\n",
      "2.4804441928863525\n",
      "2.4804441928863525\n",
      "2.4804437160491943\n",
      "2.4804434776306152\n",
      "2.4804437160491943\n",
      "2.4804437160491943\n",
      "2.480443239212036\n",
      "2.480443000793457\n",
      "2.480443000793457\n",
      "2.480442762374878\n",
      "2.4804422855377197\n",
      "2.4804422855377197\n",
      "2.4804422855377197\n",
      "2.4804420471191406\n",
      "2.4804420471191406\n",
      "2.4804418087005615\n",
      "2.4804415702819824\n",
      "2.4804415702819824\n",
      "2.4804413318634033\n",
      "2.480441093444824\n",
      "2.480440855026245\n",
      "2.480440855026245\n",
      "2.480440616607666\n",
      "2.480440378189087\n",
      "2.480440378189087\n",
      "2.480440378189087\n",
      "2.4804396629333496\n",
      "2.4804399013519287\n",
      "2.4804396629333496\n",
      "2.4804396629333496\n",
      "2.4804391860961914\n",
      "2.4804391860961914\n",
      "2.4804389476776123\n",
      "2.4804391860961914\n",
      "2.480438709259033\n",
      "2.480438709259033\n",
      "2.480438470840454\n",
      "2.480438232421875\n",
      "2.480438232421875\n",
      "2.480437994003296\n",
      "2.480437994003296\n",
      "2.480437755584717\n",
      "2.480437755584717\n",
      "2.4804372787475586\n",
      "2.4804372787475586\n",
      "2.4804370403289795\n",
      "2.4804370403289795\n",
      "2.4804370403289795\n",
      "2.4804365634918213\n",
      "2.480436325073242\n",
      "2.480436086654663\n",
      "2.480436325073242\n",
      "2.480436086654663\n",
      "2.480436086654663\n",
      "2.480436086654663\n",
      "2.480435848236084\n",
      "2.480435609817505\n",
      "2.480435848236084\n",
      "2.480435371398926\n",
      "2.4804351329803467\n",
      "2.4804351329803467\n",
      "2.4804348945617676\n",
      "2.4804351329803467\n",
      "2.4804346561431885\n",
      "2.4804346561431885\n",
      "2.4804344177246094\n",
      "2.4804344177246094\n",
      "2.4804341793060303\n",
      "2.480433940887451\n",
      "2.480433940887451\n",
      "2.480433940887451\n",
      "2.480433702468872\n",
      "2.480433464050293\n",
      "2.480433464050293\n",
      "2.480433225631714\n",
      "2.480433464050293\n",
      "2.480433225631714\n",
      "2.4804327487945557\n",
      "2.4804329872131348\n",
      "2.4804327487945557\n",
      "2.4804325103759766\n",
      "2.4804325103759766\n",
      "2.4804322719573975\n",
      "2.4804325103759766\n",
      "2.4804320335388184\n",
      "2.4804317951202393\n",
      "2.4804320335388184\n",
      "2.4804317951202393\n",
      "2.48043155670166\n",
      "2.48043155670166\n",
      "2.480431318283081\n",
      "2.480431318283081\n",
      "2.480431079864502\n",
      "2.480431079864502\n",
      "2.4804306030273438\n",
      "2.480430841445923\n",
      "2.4804303646087646\n",
      "2.4804306030273438\n",
      "2.480430841445923\n",
      "2.4804306030273438\n",
      "2.4804301261901855\n",
      "2.4804301261901855\n",
      "2.4804298877716064\n",
      "2.4804298877716064\n",
      "2.4804298877716064\n",
      "2.4804294109344482\n",
      "2.4804296493530273\n",
      "2.4804294109344482\n",
      "2.4804294109344482\n",
      "2.480429172515869\n",
      "iteration=1001 loss=2.4604\n"
     ]
    }
   ],
   "source": [
    "#Let's build out actual algorithm\n",
    "g = torch.Generator().manual_seed(10)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs , chs[1:]):\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "training_sample = xs.shape[0]\n",
    "\n",
    "x_enc = torchFunc.one_hot(xs, num_classes=27).float()\n",
    "Weights = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "Bias = torch.randn((1, 27), generator=g, requires_grad=True)\n",
    "# Weights2 = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "# Bias2 = torch.randn((1, 27), generator=g, requires_grad=True)\n",
    "# Weights3 = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "# Bias3 = torch.randn((1, 27), generator=g, requires_grad=True)\n",
    "learning_rate = -50\n",
    "learning_rate_decrease = 0.999999999999\n",
    "current_loss = 100\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    #level 1\n",
    "    logits = x_enc @ Weights \n",
    "    counts = logits.exp()\n",
    "    prob = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "    # #level 2\n",
    "    # logits = prob @ Weights2 + Bias2\n",
    "    # counts = logits.exp()\n",
    "    # prob = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "    # #level 3\n",
    "    # logits = prob @ Weights3 + Bias3\n",
    "    # counts = logits.exp()\n",
    "    # prob = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "    loss = -prob[torch.arange(training_sample), ys].log().mean() + 0.01*(Weights**2).mean()\n",
    "    current_loss = loss.item()\n",
    "    print(current_loss)\n",
    "\n",
    "    Weights.grad = None\n",
    "    Bias.grad = None\n",
    "    # Weights2.grad = None\n",
    "    # Bias2.grad = None\n",
    "    # Weights3.grad = None\n",
    "    # Bias3.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    Weights.data += learning_rate * Weights.grad\n",
    "    # Bias.data +=  learning_rate * Bias.grad\n",
    "    # Weights2.data += learning_rate * Weights2.grad\n",
    "    # Bias2.data +=  learning_rate * Bias2.grad\n",
    "    # Weights3.data += learning_rate * Weights3.grad\n",
    "    # Bias3.data +=  learning_rate * Bias3.grad\n",
    "\n",
    "    iteration += 1\n",
    "    if(iteration > 1000):\n",
    "        break\n",
    "\n",
    "\n",
    "logits = x_enc @ Weights\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdim=True)\n",
    "loss = -prob[torch.arange(training_sample), ys].log().mean()\n",
    "print(f\"{iteration=} loss={loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n",
      "merith\n",
      "ke\n",
      "uiea\n",
      "jamaliynirynin\n",
      "shaneviaceys\n",
      "zadonboraziylian\n",
      "ayn\n",
      "joe\n",
      "ailletan\n",
      "sea\n",
      "ss\n",
      "sh\n",
      "ricennshaskayineiyahnn\n",
      "gyl\n",
      "kelicaauszlenizar\n",
      "te\n",
      "dshnda\n",
      "ja\n",
      "bra\n",
      "mobren\n",
      "rely\n",
      "elfeocerolyn\n",
      "tce\n",
      "zkyarrenchouon\n",
      "an\n",
      "cedidaueryar\n",
      "coleax\n",
      "lyarosa\n",
      "lulyuryne\n",
      "celedon\n",
      "ka\n",
      "ta\n",
      "rkishaviyneirchio\n",
      "lloluraynda\n",
      "msc\n",
      "rwkeyn\n",
      "zalyak\n",
      "ahelamerlelyastr\n",
      "ighish\n",
      "sealiar\n",
      "eosh\n",
      "jaliana\n",
      "dettor\n",
      "mul\n",
      "a\n",
      "gma\n",
      "ri\n",
      "ziyorelya\n",
      "nevadena\n",
      "ie\n",
      "alisazes\n",
      "tedicelynsara\n",
      "a\n",
      "eannnn\n",
      "schj\n",
      "ewa\n",
      "ri\n",
      "lucleriaziabemya\n",
      "s\n",
      "ly\n",
      "e\n",
      "zuanzio\n",
      "zesela\n",
      "fialiayn\n",
      "lort\n",
      "h\n",
      "lan\n",
      "m\n",
      "lyaiselaraie\n",
      "malett\n",
      "jneke\n",
      "cyujy\n",
      "ad\n",
      "avo\n",
      "baseleiaun\n",
      "ll\n",
      "aja\n",
      "daleifendjayn\n",
      "kiatonain\n",
      "darighonire\n",
      "n\n",
      "amamix\n",
      "ryn\n",
      "opah\n",
      "hamo\n",
      "ligusuvinari\n",
      "ke\n",
      "man\n",
      "l\n",
      "heria\n",
      "k\n",
      "locerauwesbrishvesieynevgn\n",
      "coniinore\n",
      "an\n",
      "ty\n",
      "kahlerionalicalavaleahaxqusaali\n",
      "vow\n",
      "dh\n",
      "kon\n"
     ]
    }
   ],
   "source": [
    "# let's generate some word with the weights\n",
    "for _ in range(100):\n",
    "    current_index = 0\n",
    "    generated_word = []\n",
    "    while True:\n",
    "        input = torch.zeros((27)).float()\n",
    "        input[current_index] = 1.0\n",
    "        logits = input @ Weights\n",
    "        logits = logits.detach()\n",
    "        counts = logits.exp()\n",
    "        prob = counts / counts.sum(0, keepdim=True)\n",
    "        predicted_index = torch.multinomial(prob, 1, replacement=True, generator=g).item()\n",
    "        # print(predicted_index)\n",
    "        if predicted_index == 0:\n",
    "            break\n",
    "        generated_word.append(itos[predicted_index])\n",
    "        current_index = predicted_index\n",
    "\n",
    "    generated_word = ''.join(generated_word)\n",
    "    print(generated_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
