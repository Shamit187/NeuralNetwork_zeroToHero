{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def levenshtein_distance(word1, word2):\n",
    "    len1 = len(word1) + 1\n",
    "    len2 = len(word2) + 1\n",
    "\n",
    "    # Create the distance matrix\n",
    "    dp = [[0 for _ in range(len2)] for _ in range(len1)]\n",
    "\n",
    "    # Initialize base cases \n",
    "    for i in range(len1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len2):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    # Calculate minimum edit operations\n",
    "    for i in range(1, len1):\n",
    "        for j in range(1, len2):\n",
    "            if word1[i - 1] == word2[j - 1]:\n",
    "                cost = 0  # Substitution cost if characters match\n",
    "            else:\n",
    "                cost = 1  # Substitution cost\n",
    "\n",
    "            dp[i][j] = min(\n",
    "                dp[i - 1][j] + 1,  # Deletion\n",
    "                dp[i][j - 1] + 1,  # Insertion\n",
    "                dp[i - 1][j - 1] + cost,  # Substitution\n",
    "            )\n",
    "\n",
    "    return dp[len1 - 1][len2 - 1]  # Result at the bottom right of the matrix\n",
    "\n",
    "\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.path = None\n",
    "\n",
    "\n",
    "class AppTrie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, path):\n",
    "        current_node = self.root\n",
    "        if current_node.path is None:\n",
    "            current_node.path = path.split(\" > \")[0]\n",
    "\n",
    "        increasing_path = \"\"\n",
    "        for word in path.split(\" > \"):\n",
    "            increasing_path = increasing_path + \" > \" + word if increasing_path else word\n",
    "            if word not in current_node.children:\n",
    "                current_node.children[word] = TrieNode()\n",
    "            current_node.children[word].path = increasing_path\n",
    "            current_node = current_node.children[word]\n",
    "\n",
    "    def search(self, query, threshold=10):\n",
    "        results = []\n",
    "\n",
    "        def _dfs_query(node: TrieNode, query: str):\n",
    "            if node.children == {}:\n",
    "                return\n",
    "            \n",
    "            print(\"Searching\", node.children)\n",
    "\n",
    "            for child_word, child_node in node.children.items():\n",
    "                distance = levenshtein_distance(query, child_word)\n",
    "                print(\"Distance\", distance, child_word, query)\n",
    "                if distance <= threshold:\n",
    "                    results.append(child_node.path)\n",
    "                else :\n",
    "                    _dfs_query(child_node, query)\n",
    "        \n",
    "        _dfs_query(self.root, query)\n",
    "        return results\n",
    "    \n",
    "# Construct the trie for testing\n",
    "# trie = AppTrie()\n",
    "# trie.insert(\"select_post > new_comment > add_image\")\n",
    "# trie.insert(\"settings > battery > toggle_battery_saver\")\n",
    "# trie.insert(\"select_post > new_comment\")\n",
    "# trie.insert(\"select_post\")\n",
    "\n",
    "# print(\"output:\", trie.search(\"comment_on_image\", 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trie.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test Cases from Your Description\n",
    "# test_cases = [\n",
    "#     {\"input\":\"toggle_battery_saver\",\"output\" :\"settings > battery > toggle_battery_saver\"},\n",
    "#     {\"input\":\"turn_on_battery_saver\",\"output\":\"settings > battery > toggle_battery_saver\"},\n",
    "#     {\"input\":\"turn_of_battery_saver\",\"output\":\"settings > battery > toggle_battery_saver\"},\n",
    "#     {\"input\":\"comment_on_image\",\"output\": \"select_post > new_comment > add_image\"},\n",
    "#     {\"input\":\"comment_on_post\",\"output\": \"select_post > new_comment\"},\n",
    "#     {\"input\":\"see_post\",\"output\": \"select_post\"}\n",
    "# ]\n",
    "\n",
    "# # Run tests\n",
    "# for case in test_cases:\n",
    "#     result = trie.search(case[\"input\"])\n",
    "#     assert result == [case[\"output\"]], f\"Failed for input: {case['input']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_models = list(gensim.downloader.info()['models'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300 already exists. Skipping download.\n",
      "conceptnet-numberbatch-17-06-300 already exists. Skipping download.\n",
      "word2vec-ruscorpora-300 already exists. Skipping download.\n",
      "word2vec-google-news-300 already exists. Skipping download.\n",
      "glove-wiki-gigaword-50 already exists. Skipping download.\n",
      "glove-wiki-gigaword-100 already exists. Skipping download.\n",
      "glove-wiki-gigaword-200 already exists. Skipping download.\n",
      "glove-wiki-gigaword-300 already exists. Skipping download.\n",
      "glove-twitter-25 already exists. Skipping download.\n",
      "glove-twitter-50 already exists. Skipping download.\n",
      "glove-twitter-100 already exists. Skipping download.\n",
      "glove-twitter-200 already exists. Skipping download.\n",
      "__testing_word2vec-matrix-synopsis already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Directory where you want to save the downloaded models\n",
    "dir = \"pretrained_word2vec_gensim\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "\n",
    "models = api.info()['models'].keys()\n",
    "\n",
    "for model in models:\n",
    "    # Define the path for the model to be saved\n",
    "    model_path = os.path.join(dir, model + \".model\")\n",
    "    \n",
    "    # Check if the model file already exists\n",
    "    if not os.path.exists(model_path):\n",
    "        # Download the model only if it does not exist\n",
    "        print(f\"Downloading {model}...\")\n",
    "        embedding_file = api.load(model)\n",
    "        # Save the model locally\n",
    "        embedding_file.save(model_path)\n",
    "        print(f\"{model} has been downloaded and saved to {model_path}\")\n",
    "    else:\n",
    "        print(f\"{model} already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pretrained_word2vec_gensim/fasttext-wiki-news-subwords-300.model',\n",
       " 'pretrained_word2vec_gensim/conceptnet-numberbatch-17-06-300.model',\n",
       " 'pretrained_word2vec_gensim/word2vec-ruscorpora-300.model',\n",
       " 'pretrained_word2vec_gensim/word2vec-google-news-300.model',\n",
       " 'pretrained_word2vec_gensim/glove-wiki-gigaword-50.model',\n",
       " 'pretrained_word2vec_gensim/glove-wiki-gigaword-100.model',\n",
       " 'pretrained_word2vec_gensim/glove-wiki-gigaword-200.model',\n",
       " 'pretrained_word2vec_gensim/glove-wiki-gigaword-300.model',\n",
       " 'pretrained_word2vec_gensim/glove-twitter-25.model',\n",
       " 'pretrained_word2vec_gensim/glove-twitter-50.model',\n",
       " 'pretrained_word2vec_gensim/glove-twitter-100.model',\n",
       " 'pretrained_word2vec_gensim/glove-twitter-200.model',\n",
       " 'pretrained_word2vec_gensim/__testing_word2vec-matrix-synopsis.model']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = \"pretrained_word2vec_gensim\"\n",
    "model_paths = [os.path.join(dir, model + \".model\") for model in api.info()['models'].keys()]\n",
    "model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comment', 'on', 'image'] [['select', 'post '], [' new', 'comment '], [' add', 'image']]\n",
      "comment ['new', 'comment'] 0.72556317\n",
      "image ['add', 'image'] 0.7751524\n",
      "Time taken: 0.0006873607635498047\n"
     ]
    }
   ],
   "source": [
    "model = model_paths[0]\n",
    "\n",
    "# load model\n",
    "model = gensim.models.KeyedVectors.load(model)\n",
    "\n",
    "# query and path\n",
    "query = \"comment_on_image\"\n",
    "path = \"select_post > new_comment > add_image\"\n",
    "\n",
    "query = query.split(\"_\")\n",
    "path = [i.split(\"_\") for i in path.split(\">\")]\n",
    "print(query, path)\n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# get the most similar word of each word in the query\n",
    "for word in query:\n",
    "    word = word.strip()\n",
    "    if not word:\n",
    "        continue\n",
    "    for path_word in path:\n",
    "        path_word = [v.strip() for v in path_word]\n",
    "        if not path_word:\n",
    "            continue\n",
    "        # print(word, path_word)\n",
    "        similarity = model.n_similarity([word], path_word)\n",
    "        # print(similarity)\n",
    "        if similarity > threshold:\n",
    "            print (word, path_word, similarity)\n",
    "end = time.time()\n",
    "print(\"Time taken:\", end - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import math\n",
    "from itertools import chain\n",
    "\n",
    "#hyperparameters\n",
    "model = model_paths[0]\n",
    "model = gensim.models.KeyedVectors.load(model)\n",
    "w = 1\n",
    "b = 0\n",
    "sigma = 1\n",
    "mu = 0\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "def sim(word1, word2):\n",
    "    if word1 == word2:\n",
    "        return 1.0\n",
    "    try:\n",
    "        return sigmoid(w * model.similarity(word1, word2) + b)\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def insert_cost(word, target, string):\n",
    "    score = 0\n",
    "    for i in range(len(string)):\n",
    "        if string[i] == target:\n",
    "            continue\n",
    "        if sim(word, string[i]) > score:\n",
    "            score = sim(word, string[i])\n",
    "    \n",
    "    return 1 - sigma * score + mu\n",
    "\n",
    "def substitute_cost(word, target):\n",
    "    return 2 - 2*sim(word, target)\n",
    "\n",
    "\n",
    "# https://arxiv.org/pdf/1810.10752.pdf\n",
    "def distance_string(query, path):\n",
    "    query = query.split(\"_\")\n",
    "    path = [i.split(\"_\") for i in path.split(\">\")]\n",
    "    path = list(chain.from_iterable(path))\n",
    "    \n",
    "    # print(query, path)\n",
    "    dp = [[0 for _ in range(len(path) + 1)] for _ in range(len(query) + 1)]\n",
    "\n",
    "    # base case\n",
    "    for i in range(len(query) + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len(path) + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    # calculate minimum edit operations\n",
    "    for i in range(1, len(query) + 1):\n",
    "        for j in range(1, len(path) + 1):\n",
    "            insertion = dp[i][j - 1] + insert_cost(query[i-1], path[j-1], path)\n",
    "            deletion = dp[i - 1][j] + insert_cost(path[j-1], query[i-1], query)\n",
    "            substitution = dp[i - 1][j - 1] + substitute_cost(query[i-1], path[j-1])\n",
    "            dp[i][j] = min(insertion, deletion, substitution)\n",
    "\n",
    "    return dp[len(query)][len(path)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_star_search(self, query, threshold_lower=1, threshold_upper=1.5):\n",
    "    h = []\n",
    "    heapq.heappush(h, (0, 0, self.root))\n",
    "    results = []\n",
    "\n",
    "    while len(h) > 0:\n",
    "        current_cost, step, current_node = heapq.heappop(h)\n",
    "        a = 0.8\n",
    "        b = 0.2\n",
    "\n",
    "        for child_word, child_node in current_node.children.items():\n",
    "            distance_path = distance_string(query, child_node.path)\n",
    "            distance_current = distance_string(query, child_word)\n",
    "            distance = a * distance_path + b * distance_current\n",
    "\n",
    "            distance = distance_string(query, child_node.path)\n",
    "            distance = distance / (step + 1)\n",
    "\n",
    "            # print stat\n",
    "            # print(\"Searching in path:\", child_node.path, \"with distance:\", distance, \"and step:\", step + 1)\n",
    "\n",
    "            if distance <= threshold_upper:\n",
    "                results.append(child_node.path)\n",
    "                if distance <= threshold_lower:\n",
    "                    return results\n",
    "\n",
    "            heapq.heappush(h, (distance, step + 1, child_node))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "AppTrie.a_star_search = a_star_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'toggle_battery_saver': <__main__.TrieNode object at 0x7f0c71281220>}\n"
     ]
    }
   ],
   "source": [
    "# Construct the trie for testing\n",
    "trie = AppTrie()\n",
    "trie.insert(\"select_post > new_comment > add_image\")\n",
    "trie.insert(\"settings > battery > toggle_battery_saver\")\n",
    "trie.insert(\"select_post > new_comment\")\n",
    "trie.insert(\"select_post\")\n",
    "\n",
    "print(trie.root.children[\"settings\"].children['battery'].children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['select_post > new_comment > add_image', 'settings > battery > toggle_battery_saver']\n",
      "['select_post > new_comment > add_image', 'settings > battery > toggle_battery_saver']\n",
      "['select_post > new_comment']\n",
      "['select_post', 'select_post > new_comment']\n",
      "['select_post']\n"
     ]
    }
   ],
   "source": [
    "test_input = [\n",
    "    \"turn_on_battery_saver\",\n",
    "    \"turn_of_battery_saver\",\n",
    "    \"comment_on_image\",\n",
    "    \"comment_on_post\",\n",
    "    \"see_post\"\n",
    "]\n",
    "\n",
    "for case in test_input:\n",
    "    result = trie.a_star_search(case)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
